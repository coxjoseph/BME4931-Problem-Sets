{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4863418",
   "metadata": {},
   "source": [
    "# Problem Set 10 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c209eeb9",
   "metadata": {},
   "source": [
    "This code covers the implementation of a Long Short-Term Memory (LSTM) architecture. Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) architecture designed to address the vanishing gradient problem in standard RNNs. It is particularly well-suited for processing sequences of data, making it a popular choice for tasks such as time series analysis, natural language processing, and more.\n",
    "\n",
    "#### A brief note: \n",
    "As we move through the latter half of the Deep Learning module, we will be covering some of the newer, state-of-the-art models and architectures. These models become increasingly complex and require some higher-level coding insights and ability to create from scratch. As you may have noticed, implementations of LSTMs and RNNs are not on Homework 2. This doesn't mean they aren't important! \n",
    "\n",
    "This Problem Set will be largely *understanding* based - not asking as much \"coding\" questions as the previous few. Of course, as we walk through the code feel free to ask any questions about the code itself.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570c43d9",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74198e56",
   "metadata": {},
   "source": [
    "The data from this Problem Set is the training set of [The 2012 PhysioNet/Computing in Cardiology Challenge](https://physionet.org/content/challenge-2012/1.0.0/). The goal of the challenge (and this model) is to aid in the prediction of mortaility rates in the ICU. Though the challenge offers multiple possible targets, in this code we focus on **Length of Stay** (LOS). \n",
    "\n",
    "Predicting LOS is valuable for many reasons: it aids in resource allocation and planning, helping healthcare providers allocate beds, staffing, and equipment efficiently, ensuring that critical care resources are available when needed \\[[1](https://onlinelibrary.wiley.com/doi/full/10.1111/imj.14962)\\]. LOS prediction can improve patient care and outcomes by allowing medical teams to make timely decisions regarding treatment, discharge, or transfer to lower-acuity settings \\[[2](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1067452/)\\]. It can help identify patients at risk of prolonged stays, enabling interventions to prevent complications and reduce healthcare costs. Furthermore, LOS prediction can facilitate hospital management, optimize bed turnover, and reduce wait times, ultimately benefiting both patients and healthcare institutions \\[[3](https://doi.org/10.1097/MLR.0000000000001596)\\].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aa2fac",
   "metadata": {},
   "source": [
    "This dataset consists of records from 4,000 ICU stays. All patients were adults and stayed for at least 48 hours. **A great deal of preprocessing has been performed on this data to make it suitable for LSTM.**\n",
    "\n",
    "In order, the steps performed are:\n",
    "1. Loading the data from its original form (a large folder of csv files of the form `ID.txt`, one for each patient) into one larger csv called `patient_data.csv` containingthe columns Patient_ID, Time, Parameter, Value. (Script: `icu_data.py`)\n",
    "2. Loading this new csv and subsetting 5 of the original 37 features: SaO2, NIDiasABP, HR, Urine, PaO2\n",
    "    - These features were chosen at random\n",
    "3. Performing numerical/type conversion for the data: changing Time from the form HH:MM to \"minutes\" and ensuring all data is a native type in Python\n",
    "4. Aggregating the data - converting the columns from Patient_ID, Time, Parameter, Value to Patient_ID, Parameter, Time 0, Time 1, Time 2, ... Time 2880\n",
    "5. Selecting only patients that had measurements for *all* 5 of our features\n",
    "6. Filling the NaN values in our timepoints with backfill (and forward fill for our final timepoints).\n",
    "6. Saving as a JSON file with key/value pairs of Patient_ID/time-series data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ed9b46",
   "metadata": {},
   "source": [
    "**What issues may there be with this preprocessing, if any? How might you fix them?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971fcf2f",
   "metadata": {},
   "source": [
    "[Type Here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aebdb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76ca7ad",
   "metadata": {},
   "source": [
    "I have saved the patient data into a json file called `patient_data.json` (it should, if you cloned the GitHub, exist in the `data` directory relative to our current one). **Load this data into a variable called `patient_data`. Print the number of entries in this dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3db39c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eda91ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the patient outcomes (the Length of stay):\n",
    "outcomes = pd.read_csv('data/outcomes.csv')\n",
    "outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d6b003",
   "metadata": {},
   "source": [
    "This data has 4000 rows and 2 columns. **Is this different than the number of items in our `patient_data` variable? If so, why? How can we fix it?** (Hint: it is different)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca665b88",
   "metadata": {},
   "source": [
    "[Type Here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a129711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the targets by removing unused IDs and preparing the features array\n",
    "## Start with two empty arrays\n",
    "features = []\n",
    "targets = []\n",
    "\n",
    "\n",
    "for patient_id in patient_data.keys():  # Recall: keys are our patient ids\n",
    "    outcome = outcomes.Length_of_stay[outcomes.RecordID == int(patient_id)]  # Take LOS where the record matches\n",
    "    feature_array = np.array(patient_data[patient_id])                       # Take features where record matches\n",
    "    # Add to our arrays\n",
    "    targets.append(int(outcome))                                        \n",
    "    features.append(feature_array)\n",
    "    \n",
    "# Convert from python lists to numpy arrays.\n",
    "features = np.array(features)\n",
    "targets = np.array(targets)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4753f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe1cb08",
   "metadata": {},
   "source": [
    "The output from cell 3 shows that our feature data is 3 dimensional: 1394x5x2881. **What do each of these dimensions represent? What is the interpretaion of the value `features[0, 1, 2]`?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c342d59",
   "metadata": {},
   "source": [
    "[Type Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274ce38f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de071623",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82b4a42",
   "metadata": {},
   "source": [
    "I will be using Pytorch's API for this Problem Set, though tensorflow will of course be somewhat similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db04ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26ea45e",
   "metadata": {},
   "source": [
    "Looking ahead at the documentation, I noticed that torch's LSTM module expects data to be in the form $\\left(\\textrm{Number of Participants} \\times \\textrm{Sequence Length} \\times \\textrm{Number of Features}\\right)$. \n",
    "\n",
    "**Create a variable `X` with the type `torch.Tensor`, and use the Tensor method `permute` to change the dimensions.\n",
    "Verify that the shape has changed by printing off `X.shape`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9482fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2ac91",
   "metadata": {},
   "source": [
    "Y is already the right shape, so I'll just convert that to a Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a201b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor(targets)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec995b06",
   "metadata": {},
   "source": [
    "Of course, we need to split the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46324835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "X_train = (X_train - X_train.mean()) / X_train.std()\n",
    "X_test = (X_test - X_train.mean()) / X_train.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d885a387",
   "metadata": {},
   "source": [
    "### Defining an LSTM model.\n",
    "\n",
    "PyTorch has implemented an LSTM layer so we don't have to do a lot of the hard work. We have a bit of Object Oriented Programming ahead with the class definition. \n",
    "\n",
    "As a reminder from our intro to python, the `class` in python contains a collection of functions and values of our choosing. When creating a class, we can specifiy and access these values through the `self` variable given to each class function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23af3090",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    # The method that is called when an object is created with this class\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        # Start with pytorch's model initialization - this is called Superclassing\n",
    "        super().__init__() \n",
    "        \n",
    "        # Use our passed parameters to create a class variable lstm\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # One fully connected layer that takes our output of the LSTM and produces a prediction\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "    # The method that is called when we do the \"foward pass\" of our model: features in -> predictions out\n",
    "    def forward(self, x):\n",
    "        # LSTM layers return a tuple, and we only care about the output of the layer.\n",
    "        out, _ = self.lstm(x)  # The _ is pythons way of ignoring an output. \n",
    "        \n",
    "        out = out[:, -1, :]  # Take the last value in the output of our LSTM (the output of the last \"block\")\n",
    "        out = self.fc(out)   # Pass though our FC layer\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34357d53",
   "metadata": {},
   "source": [
    "In our forward pass, we \"throw out\" the second part of a tuple returned by the LSTM layer. Looking at the documentation, we are throwing out two variables (joined in a tuple) called `h_n` and `c_n`. **Using your knowledge of the components of LSTM, what is this part we are throwing out?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10ca5c0",
   "metadata": {},
   "source": [
    "[Type Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90450428",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58201ad3",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0284b43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 5         # Number of features in\n",
    "hidden_size = 64       # Number of output neurons for our LSTM model\n",
    "num_layers = 2         # How many LSTM layers to add \n",
    "output_size = 1        # Number of features out\n",
    "learning_rate = 0.001  # How fast the model trains\n",
    "num_epochs = 10        # How many times to loop through the training data      \n",
    "batch_size = 32        # How many training samples to work through before updating our weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9579c33e",
   "metadata": {},
   "source": [
    "Which of these can we change? Which are dependent on our data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7771a9",
   "metadata": {},
   "source": [
    "[Type Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a915c332",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b3af04",
   "metadata": {},
   "source": [
    "### Preparing for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb187bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size)  # Instance of our defined class!\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36d8c83",
   "metadata": {},
   "source": [
    "#### Datasets and Dataloaders:\n",
    "Datasets and Dataloaders are pytorch's optimized way of loading in data. Though generally unnecessary for the class, those of you with a desire to implement some of the more advanced ML algorithms in your own projects may benefit from this efficiency. Generally speaking, a Dataset will \"pair up\" your input features and targets and provide instructions for loading it into memory (either VRAM on a GPU or RAM on a CPU) and a DataLoader will allow you to access the data in batches using python's `in` syntax. In more advanced applications when you want to perform augmentation to your data, you can do so in the DataLoader through the `transforms` keyword argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a642c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e646c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "epochs = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, targets.view(-1, 1))  # Reshape targets to the same shape as our output\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update our loss. loss.item() extracts the actual value of our loss (as a float)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for the current epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader)}')\n",
    "    losses.append(total_loss/len(train_loader))\n",
    "    epochs.append(epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441d61c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Forward pass on the test data\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "\n",
    "# Calculate the loss (MSE)\n",
    "criterion = nn.MSELoss()\n",
    "test_loss = criterion(test_outputs, y_test.view(-1, 1))  # Reshape y_test like in our training loop\n",
    "\n",
    "# Convert to numpy arrays\n",
    "test_outputs = test_outputs.numpy()\n",
    "y_test = y_test.numpy()\n",
    "\n",
    "\n",
    "# Print or return the test loss\n",
    "print(f'Test Loss (MSE): {test_loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9010556",
   "metadata": {},
   "source": [
    "**Evauluate the model. Is it overfitting or underfitting? Besides MSE, what are some other metrics, plots, or methods we may want to calculate/implement to evaluate our model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daca22c2",
   "metadata": {},
   "source": [
    "[Type Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a075849",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
