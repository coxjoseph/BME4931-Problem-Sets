{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d01b17db",
   "metadata": {},
   "source": [
    "# Neural Network Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764da66a",
   "metadata": {},
   "source": [
    "Since the homework is due the same week we discuss neural networks, I've created this short file to demonstrate how to create, train, and predict with simple neural networks. We will discuss a lot of the \"what\" and \"why\" behind them in class, but I wanted to send out a file with some examples you can (hopefully) make use of when implementing them in your homework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebe7ce2",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd29c5",
   "metadata": {},
   "source": [
    "In this notebook, I will give one example for classification and one for regression. I won't go too much into a lot of the generic pre-processing required (like loading the datasets or encoding them) but instead will focus on the Neural Network implementation. I will use `Pytorch` to create my neural networks. You're welcome to use `TensorFlow` in your code as well. A helpful tutorial for creating networks in TensorFlow can be found [here](https://www.tensorflow.org/tutorials/quickstart/beginner)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65a3f4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da2e66",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f694e07b",
   "metadata": {},
   "source": [
    "We will perform classification on the Wisconsin Breast Cancer Dataset (source: UCI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b6785b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           name     role         type demographic  \\\n",
      "0            Sample_code_number       ID  Categorical        None   \n",
      "1               Clump_thickness  Feature      Integer        None   \n",
      "2       Uniformity_of_cell_size  Feature      Integer        None   \n",
      "3      Uniformity_of_cell_shape  Feature      Integer        None   \n",
      "4             Marginal_adhesion  Feature      Integer        None   \n",
      "5   Single_epithelial_cell_size  Feature      Integer        None   \n",
      "6                   Bare_nuclei  Feature      Integer        None   \n",
      "7               Bland_chromatin  Feature      Integer        None   \n",
      "8               Normal_nucleoli  Feature      Integer        None   \n",
      "9                       Mitoses  Feature      Integer        None   \n",
      "10                        Class   Target       Binary        None   \n",
      "\n",
      "                  description units missing_values  \n",
      "0                        None  None             no  \n",
      "1                        None  None             no  \n",
      "2                        None  None             no  \n",
      "3                        None  None             no  \n",
      "4                        None  None             no  \n",
      "5                        None  None             no  \n",
      "6                        None  None            yes  \n",
      "7                        None  None             no  \n",
      "8                        None  None             no  \n",
      "9                        None  None             no  \n",
      "10  2 = benign, 4 = malignant  None             no  \n"
     ]
    }
   ],
   "source": [
    "# fetch dataset \n",
    "breast_cancer_wisconsin = fetch_ucirepo(id=15) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = breast_cancer_wisconsin.data.features\n",
    "y = breast_cancer_wisconsin.data.targets\n",
    "  \n",
    "# variable information \n",
    "print(breast_cancer_wisconsin.variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26a7476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df containes missing values, correcting that:\n",
    "na_mask = X.isna().any(axis=1)\n",
    "not_na = np.logical_not(na_mask)\n",
    "X = X[not_na]\n",
    "y = y[not_na]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a4596f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(683, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clump_thickness</th>\n",
       "      <th>Uniformity_of_cell_size</th>\n",
       "      <th>Uniformity_of_cell_shape</th>\n",
       "      <th>Marginal_adhesion</th>\n",
       "      <th>Single_epithelial_cell_size</th>\n",
       "      <th>Bare_nuclei</th>\n",
       "      <th>Bland_chromatin</th>\n",
       "      <th>Normal_nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clump_thickness  Uniformity_of_cell_size  Uniformity_of_cell_shape  \\\n",
       "0                5                        1                         1   \n",
       "1                5                        4                         4   \n",
       "2                3                        1                         1   \n",
       "3                6                        8                         8   \n",
       "4                4                        1                         1   \n",
       "\n",
       "   Marginal_adhesion  Single_epithelial_cell_size  Bare_nuclei  \\\n",
       "0                  1                            2          1.0   \n",
       "1                  5                            7         10.0   \n",
       "2                  1                            2          2.0   \n",
       "3                  1                            3          4.0   \n",
       "4                  3                            2          1.0   \n",
       "\n",
       "   Bland_chromatin  Normal_nucleoli  Mitoses  \n",
       "0                3                1        1  \n",
       "1                3                2        1  \n",
       "2                3                1        1  \n",
       "3                3                7        1  \n",
       "4                3                1        1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "134582ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class\n",
       "0      2\n",
       "1      2\n",
       "2      2\n",
       "3      2\n",
       "4      2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9534e687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>683 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Class\n",
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "..     ...\n",
       "694      0\n",
       "695      0\n",
       "696      1\n",
       "697      1\n",
       "698      1\n",
       "\n",
       "[683 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace 2 with 0 and 4 with 1 in y\n",
    "y = y.replace(2, 0).replace(4, 1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7d3b409",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9e410a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to convert our data to pytorch's Tensor objects using a combination of pandas to_numpy\n",
    "#  and pytorch's from_numpy. We call .to(torch.float32) because the model expects data to be 32-bit floats\n",
    "#  not 64 bit.\n",
    "X_train = torch.from_numpy(X_train.to_numpy()).to(torch.float32)\n",
    "y_train = torch.from_numpy(y_train.to_numpy()).to(torch.float32)\n",
    "X_test = torch.from_numpy(X_test.to_numpy()).to(torch.float32)\n",
    "y_test = torch.from_numpy(y_test.to_numpy()).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dde353a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreastCancerClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Call super().__init__() always (it does behind-the-scenes work for the model)\n",
    "        super().__init__()\n",
    "        \n",
    "        # We now create the layers of our model:\n",
    "        \n",
    "        # The first number is the initial number of features in the data (9). \n",
    "        # The second is a hyperparameter, setting the number of features in the first hidden layer. I chose 18.\n",
    "        self.hidden_1 = nn.Linear(9, 18)  \n",
    "        self.activation_1 = nn.ReLU()  # Activation layer between each hidden layer\n",
    "        \n",
    "        # The first is the number at the end of the first linear layer. The second is again a hyperparameter.\n",
    "        self.hidden_2 = nn.Linear(18, 9)  \n",
    "        self.activation_2 = nn.ReLU() # Activation layer between each hidden layer\n",
    "        \n",
    "        # Finally, we take the number at the end of the second linear layer and map to 1 output neuron \n",
    "        # (our predictor). Sigmoid then takes our predictor and fits it to a probability distribution\n",
    "        # that is 0 for low values and 1 for high values\n",
    "        self.output = nn.Linear(9, 1)  \n",
    "        self.predictions = nn.Sigmoid()  # Fits to a binary probability distribution.\n",
    "        # In the end, we chained fully connected layers, interleaving activation layers between each\n",
    "        # We went from 9 features, to 18, back to 9, then to 1.\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # The forward method is required: it tells pytorch what to do when features are passed into it.\n",
    "        # Here, we make use of the linear layers we created above.\n",
    "        x = self.activation_1(self.hidden_1(x))  # 9 -> 18\n",
    "        x = self.activation_2(self.hidden_2(x))  # 18 -> 9\n",
    "        x = self.predictions(self.output(x))     # 9 -> 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bc0b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = BreastCancerClassifier()  # initialize a model\n",
    "loss_fn = nn.BCELoss()  # Define the objective for our model: minimize Binary Cross Entropy (classification only)\n",
    "optimizer = optim.Adam(classifier_model.parameters(), lr=0.001)  # Lets the model learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ea64756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 0.7303469181060791\n",
      "Epoch 2: loss 0.6985543966293335\n",
      "Epoch 3: loss 0.6678364872932434\n",
      "Epoch 4: loss 0.6256605982780457\n",
      "Epoch 5: loss 0.5562031269073486\n",
      "Epoch 6: loss 0.46026185154914856\n",
      "Epoch 7: loss 0.35547372698783875\n",
      "Epoch 8: loss 0.26384228467941284\n",
      "Epoch 9: loss 0.19551783800125122\n",
      "Epoch 10: loss 0.1549387127161026\n",
      "Epoch 11: loss 0.12886878848075867\n",
      "Epoch 12: loss 0.11097212135791779\n",
      "Epoch 13: loss 0.09781638532876968\n",
      "Epoch 14: loss 0.08776700496673584\n",
      "Epoch 15: loss 0.08072865754365921\n",
      "Epoch 16: loss 0.07564664632081985\n",
      "Epoch 17: loss 0.0719011127948761\n",
      "Epoch 18: loss 0.06922096759080887\n",
      "Epoch 19: loss 0.06754116714000702\n",
      "Epoch 20: loss 0.0664314478635788\n",
      "Epoch 21: loss 0.06633865833282471\n",
      "Epoch 22: loss 0.06633733958005905\n",
      "Epoch 23: loss 0.06753381341695786\n",
      "Epoch 24: loss 0.0689278319478035\n",
      "Epoch 25: loss 0.06996837258338928\n",
      "Epoch 26: loss 0.07180933654308319\n",
      "Epoch 27: loss 0.07332177460193634\n",
      "Epoch 28: loss 0.07532535493373871\n",
      "Epoch 29: loss 0.07652923464775085\n",
      "Epoch 30: loss 0.07858292758464813\n",
      "Epoch 31: loss 0.07970111072063446\n",
      "Epoch 32: loss 0.08177454024553299\n",
      "Epoch 33: loss 0.0832689180970192\n",
      "Epoch 34: loss 0.08321321755647659\n",
      "Epoch 35: loss 0.08332734555006027\n",
      "Epoch 36: loss 0.08511174470186234\n",
      "Epoch 37: loss 0.08631256222724915\n",
      "Epoch 38: loss 0.0874733030796051\n",
      "Epoch 39: loss 0.08898015320301056\n",
      "Epoch 40: loss 0.09133338183164597\n",
      "Epoch 41: loss 0.09196959435939789\n",
      "Epoch 42: loss 0.09293060749769211\n",
      "Epoch 43: loss 0.09406549483537674\n",
      "Epoch 44: loss 0.09444394707679749\n",
      "Epoch 45: loss 0.09479734301567078\n",
      "Epoch 46: loss 0.09529399871826172\n",
      "Epoch 47: loss 0.09644719958305359\n",
      "Epoch 48: loss 0.09609368443489075\n",
      "Epoch 49: loss 0.09359382092952728\n",
      "Epoch 50: loss 0.09493294358253479\n",
      "Epoch 51: loss 0.09384094923734665\n",
      "Epoch 52: loss 0.09387298673391342\n",
      "Epoch 53: loss 0.09299726039171219\n",
      "Epoch 54: loss 0.09361997246742249\n",
      "Epoch 55: loss 0.09391909092664719\n",
      "Epoch 56: loss 0.09232992678880692\n",
      "Epoch 57: loss 0.09024449437856674\n",
      "Epoch 58: loss 0.09097223728895187\n",
      "Epoch 59: loss 0.09058883786201477\n",
      "Epoch 60: loss 0.08890540897846222\n",
      "Epoch 61: loss 0.08843673020601273\n",
      "Epoch 62: loss 0.08767470717430115\n",
      "Epoch 63: loss 0.08720032870769501\n",
      "Epoch 64: loss 0.0855976864695549\n",
      "Epoch 65: loss 0.08497098833322525\n",
      "Epoch 66: loss 0.08407112956047058\n",
      "Epoch 67: loss 0.08214175701141357\n",
      "Epoch 68: loss 0.08118817210197449\n",
      "Epoch 69: loss 0.08011483401060104\n",
      "Epoch 70: loss 0.07920431345701218\n",
      "Epoch 71: loss 0.07816124707460403\n",
      "Epoch 72: loss 0.07553593069314957\n",
      "Epoch 73: loss 0.07456522434949875\n",
      "Epoch 74: loss 0.07537350803613663\n",
      "Epoch 75: loss 0.07284335792064667\n",
      "Epoch 76: loss 0.07294457405805588\n",
      "Epoch 77: loss 0.07055681198835373\n",
      "Epoch 78: loss 0.06841826438903809\n",
      "Epoch 79: loss 0.06570286303758621\n",
      "Epoch 80: loss 0.0690918117761612\n",
      "Epoch 81: loss 0.0663251131772995\n",
      "Epoch 82: loss 0.06549959629774094\n",
      "Epoch 83: loss 0.062453169375658035\n",
      "Epoch 84: loss 0.06627906113862991\n",
      "Epoch 85: loss 0.059658799320459366\n",
      "Epoch 86: loss 0.06466030329465866\n",
      "Epoch 87: loss 0.05814283341169357\n",
      "Epoch 88: loss 0.06238313391804695\n",
      "Epoch 89: loss 0.05694923177361488\n",
      "Epoch 90: loss 0.05695752799510956\n",
      "Epoch 91: loss 0.055678870528936386\n",
      "Epoch 92: loss 0.05778159946203232\n",
      "Epoch 93: loss 0.054460808634757996\n",
      "Epoch 94: loss 0.05393362045288086\n",
      "Epoch 95: loss 0.05262235179543495\n",
      "Epoch 96: loss 0.05384158715605736\n",
      "Epoch 97: loss 0.05059463158249855\n",
      "Epoch 98: loss 0.05509278550744057\n",
      "Epoch 99: loss 0.04990508407354355\n",
      "Epoch 100: loss 0.052450940012931824\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100  # How long to train for (how many times to loop through the training set)\n",
    "batch_size = 13  # How many training items before learning (updating model weights)\n",
    "\n",
    "# A test (if you are copying this code, you need the size of the training data to be evenly split by your\n",
    "#   batch size, or you will have an index error. As indicated by the error I create, you can also avoid this \n",
    "#   requirement by dividing into batches, and then on the last batch just use the rest of the training data.\n",
    "#    \n",
    "#   If your training data does not have a lot of factors, prefer smaller factors (1, 2) over larger ones. 10 is \n",
    "#   generally a good number to be close to.\n",
    "if not len(X_train) % batch_size == 0:\n",
    "    raise ValueError('Select a batch size that evenly divides your data'\n",
    "                    '(or code yourself: make the final batch contain the rest'\n",
    "                    'of the elements in the set in the second loop below)')\n",
    "    \n",
    "\n",
    "# Two for loops: one for every epoch, and one for every batch in the training set.\n",
    "#    This means we train on every sample on the training set once for every epoch.\n",
    "for epoch in range(n_epochs):\n",
    "    for batch in range(0, len(X_train), batch_size):  # range(start, stop, step)\n",
    "        X_batch = X_train[batch:batch+batch_size]     # from our start index to our start + step size\n",
    "        y_pred = classifier_model(X_batch)            # create predictions based on the current weights\n",
    "        true_batch = y_train[batch:batch+batch_size]  # take the actual values from our y_train data\n",
    "        loss = loss_fn(y_pred, true_batch)            # calculate how far off we were using our BCE loss function\n",
    "        \n",
    "        # Update our weights\n",
    "        optimizer.zero_grad()                         # Zero out gradients (otherwise they are saved btwn batches)\n",
    "        loss.backward()                               # Propogate our loss (more on this in that module)\n",
    "        optimizer.step()                              # Update our weights with the loss.\n",
    "    print(f'Epoch {epoch+1}: loss {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edc4481f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained classifier achieved accuracy 0.9416058659553528 on the testing set\n"
     ]
    }
   ],
   "source": [
    "# with torch.no_grad() is technically optional here, but good practice for when using validation\n",
    "with torch.no_grad():\n",
    "    predictions = classifier_model(X_test)  # Predict outputs with our trained weights on our test data\n",
    "\n",
    "# Calculate accuracy score (fancy trick: because we map from 0 to 1 we can round and then compare to y_test \n",
    "#    with ==. In other words, predictions.round() picks the closer of 0 or 1 for our data, and we compare that \n",
    "#    to y_test to get a boolean array [True False True True ...], which we convert to 0 and 1s again and take the \n",
    "#    mean.)\n",
    "accuracy = (predictions.round() == y_test).float().mean()\n",
    "print(f'Trained classifier achieved accuracy {accuracy} on the testing set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082b7d1e",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac3862b",
   "metadata": {},
   "source": [
    "Regression will not change very much from classification - we will use a regression-specific loss function, and modify our output layers! We will be using the BUPA dataset from UCI for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "182c1f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       name     role         type demographic  \\\n",
      "0       mcv  Feature   Continuous        None   \n",
      "1   alkphos  Feature   Continuous        None   \n",
      "2      sgpt  Feature   Continuous        None   \n",
      "3      sgot  Feature   Continuous        None   \n",
      "4   gammagt  Feature   Continuous        None   \n",
      "5    drinks   Target   Continuous        None   \n",
      "6  selector    Other  Categorical        None   \n",
      "\n",
      "                                         description units missing_values  \n",
      "0                            mean corpuscular volume  None             no  \n",
      "1                               alkaline phosphotase  None             no  \n",
      "2                           alanine aminotransferase  None             no  \n",
      "3                         aspartate aminotransferase  None             no  \n",
      "4                      gamma-glutamyl transpeptidase  None             no  \n",
      "5  number of half-pint equivalents of alcoholic b...  None             no  \n",
      "6  field created by the BUPA researchers to split...  None             no  \n"
     ]
    }
   ],
   "source": [
    "# fetch dataset \n",
    "liver_disorders = fetch_ucirepo(id=60) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = liver_disorders.data.features \n",
    "y = liver_disorders.data.targets \n",
    "  \n",
    "# variable information \n",
    "print(liver_disorders.variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "801f211b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mcv</th>\n",
       "      <th>alkphos</th>\n",
       "      <th>sgpt</th>\n",
       "      <th>sgot</th>\n",
       "      <th>gammagt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85</td>\n",
       "      <td>92</td>\n",
       "      <td>45</td>\n",
       "      <td>27</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85</td>\n",
       "      <td>64</td>\n",
       "      <td>59</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86</td>\n",
       "      <td>54</td>\n",
       "      <td>33</td>\n",
       "      <td>16</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91</td>\n",
       "      <td>78</td>\n",
       "      <td>34</td>\n",
       "      <td>24</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87</td>\n",
       "      <td>70</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>99</td>\n",
       "      <td>75</td>\n",
       "      <td>26</td>\n",
       "      <td>24</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>96</td>\n",
       "      <td>69</td>\n",
       "      <td>53</td>\n",
       "      <td>43</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>98</td>\n",
       "      <td>77</td>\n",
       "      <td>55</td>\n",
       "      <td>35</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>91</td>\n",
       "      <td>68</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>98</td>\n",
       "      <td>99</td>\n",
       "      <td>57</td>\n",
       "      <td>45</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>345 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mcv  alkphos  sgpt  sgot  gammagt\n",
       "0     85       92    45    27       31\n",
       "1     85       64    59    32       23\n",
       "2     86       54    33    16       54\n",
       "3     91       78    34    24       36\n",
       "4     87       70    12    28       10\n",
       "..   ...      ...   ...   ...      ...\n",
       "340   99       75    26    24       41\n",
       "341   96       69    53    43      203\n",
       "342   98       77    55    35       89\n",
       "343   91       68    27    26       14\n",
       "344   98       99    57    45       65\n",
       "\n",
       "[345 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e35c251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drinks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>345 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     drinks\n",
       "0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0.0\n",
       "4       0.0\n",
       "..      ...\n",
       "340    12.0\n",
       "341    12.0\n",
       "342    15.0\n",
       "343    16.0\n",
       "344    20.0\n",
       "\n",
       "[345 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bacce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e3eef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we need to convert to pytorch's Tensor objects using a combination of pandas to_numpy\n",
    "#  and pytorch's from_numpy. We call .to(torch.float32) because the model expects data to be 32-bit floats\n",
    "#  not 64 bit.\n",
    "X_train = torch.from_numpy(X_train.to_numpy()).to(torch.float32)\n",
    "y_train = torch.from_numpy(y_train.to_numpy()).to(torch.float32)\n",
    "X_test = torch.from_numpy(X_test.to_numpy()).to(torch.float32)\n",
    "y_test = torch.from_numpy(y_test.to_numpy()).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f73d4dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BupaRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Call super().__init__() always (it does behind-the-scenes work for the model)\n",
    "        super().__init__()\n",
    "        \n",
    "        # We now create the layers of our model:\n",
    "        \n",
    "        # We still have to first use the number of features in our input: 5\n",
    "        # The second is still a hyperparameter. This time I chose 15\n",
    "        self.hidden_1 = nn.Linear(5, 15)  \n",
    "        self.activation_1 = nn.ReLU()  # Activation layer between each hidden layer\n",
    "        \n",
    "        # The first is the number at the end of the first linear layer. The second is again a hyperparameter.\n",
    "        self.hidden_2 = nn.Linear(15, 8)  \n",
    "        self.activation_2 = nn.ReLU() # Activation layer between each hidden layer\n",
    "        \n",
    "        # Finally, we take the number at the end of the second linear layer and map to 1 output neuron \n",
    "        # (our predictor). We no longer need the sigmoid here - our output is already the predictor \n",
    "        # (no need to map to a probability distribution from 0 to 1)\n",
    "        self.output = nn.Linear(8, 1)  \n",
    "\n",
    "        # In the end, we chained fully connected layers, interleaving activation layers between each\n",
    "        # We went from 5 features, to 15, to 8, then to 1.\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # The forward method is required: it tells pytorch what to do when features are passed into it.\n",
    "        # Here, we make use of the linear layers we created above.\n",
    "        x = self.activation_1(self.hidden_1(x))  # 5 -> 15\n",
    "        x = self.activation_2(self.hidden_2(x))  # 15 -> 8\n",
    "        x = self.output(x)                       # 8 -> 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c794192",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_model = BupaRegressor()  # Initialize our model\n",
    "loss_fn = nn.MSELoss()  # We need to use a loss function that makes sense for regression like MSE\n",
    "optimizer = optim.Adam(regressor_model.parameters(), lr=0.001)  # Lets our model learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ff6e44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 8.95711612701416\n",
      "Epoch 2: loss 6.778415203094482\n",
      "Epoch 3: loss 6.246530055999756\n",
      "Epoch 4: loss 5.6820149421691895\n",
      "Epoch 5: loss 5.2181477546691895\n",
      "Epoch 6: loss 4.806542873382568\n",
      "Epoch 7: loss 4.64403772354126\n",
      "Epoch 8: loss 4.45141077041626\n",
      "Epoch 9: loss 4.3446736335754395\n",
      "Epoch 10: loss 4.262369632720947\n",
      "Epoch 11: loss 4.284063339233398\n",
      "Epoch 12: loss 4.1806793212890625\n",
      "Epoch 13: loss 4.1005682945251465\n",
      "Epoch 14: loss 4.059868812561035\n",
      "Epoch 15: loss 3.9484708309173584\n",
      "Epoch 16: loss 4.017343044281006\n",
      "Epoch 17: loss 4.013096332550049\n",
      "Epoch 18: loss 3.9766674041748047\n",
      "Epoch 19: loss 3.967500925064087\n",
      "Epoch 20: loss 3.9358203411102295\n",
      "Epoch 21: loss 3.9204752445220947\n",
      "Epoch 22: loss 3.84529185295105\n",
      "Epoch 23: loss 3.8084707260131836\n",
      "Epoch 24: loss 3.8243472576141357\n",
      "Epoch 25: loss 3.8625853061676025\n",
      "Epoch 26: loss 3.783918619155884\n",
      "Epoch 27: loss 3.767948865890503\n",
      "Epoch 28: loss 3.7610976696014404\n",
      "Epoch 29: loss 3.7294747829437256\n",
      "Epoch 30: loss 3.7525370121002197\n",
      "Epoch 31: loss 3.765326499938965\n",
      "Epoch 32: loss 3.73009991645813\n",
      "Epoch 33: loss 3.7087981700897217\n",
      "Epoch 34: loss 3.699143171310425\n",
      "Epoch 35: loss 3.723815679550171\n",
      "Epoch 36: loss 3.7150065898895264\n",
      "Epoch 37: loss 3.689957857131958\n",
      "Epoch 38: loss 3.678487777709961\n",
      "Epoch 39: loss 3.6604807376861572\n",
      "Epoch 40: loss 3.6367557048797607\n",
      "Epoch 41: loss 3.650461196899414\n",
      "Epoch 42: loss 3.673492670059204\n",
      "Epoch 43: loss 3.6543588638305664\n",
      "Epoch 44: loss 3.618556022644043\n",
      "Epoch 45: loss 3.599652051925659\n",
      "Epoch 46: loss 3.584618330001831\n",
      "Epoch 47: loss 3.6307623386383057\n",
      "Epoch 48: loss 3.6515369415283203\n",
      "Epoch 49: loss 3.607015609741211\n",
      "Epoch 50: loss 3.565293550491333\n",
      "Epoch 51: loss 3.547227621078491\n",
      "Epoch 52: loss 3.6048476696014404\n",
      "Epoch 53: loss 3.5888125896453857\n",
      "Epoch 54: loss 3.5406863689422607\n",
      "Epoch 55: loss 3.500838041305542\n",
      "Epoch 56: loss 3.469787359237671\n",
      "Epoch 57: loss 3.519289970397949\n",
      "Epoch 58: loss 3.5167016983032227\n",
      "Epoch 59: loss 3.473888397216797\n",
      "Epoch 60: loss 3.429858446121216\n",
      "Epoch 61: loss 3.3974149227142334\n",
      "Epoch 62: loss 3.4667046070098877\n",
      "Epoch 63: loss 3.4383652210235596\n",
      "Epoch 64: loss 3.385636568069458\n",
      "Epoch 65: loss 3.3551082611083984\n",
      "Epoch 66: loss 3.3257713317871094\n",
      "Epoch 67: loss 3.411332130432129\n",
      "Epoch 68: loss 3.403304100036621\n",
      "Epoch 69: loss 3.3383524417877197\n",
      "Epoch 70: loss 3.2889058589935303\n",
      "Epoch 71: loss 3.269392728805542\n",
      "Epoch 72: loss 3.377636671066284\n",
      "Epoch 73: loss 3.3790833950042725\n",
      "Epoch 74: loss 3.302494764328003\n",
      "Epoch 75: loss 3.253605842590332\n",
      "Epoch 76: loss 3.2382028102874756\n",
      "Epoch 77: loss 3.370223045349121\n",
      "Epoch 78: loss 3.342693567276001\n",
      "Epoch 79: loss 3.2718665599823\n",
      "Epoch 80: loss 3.2356975078582764\n",
      "Epoch 81: loss 3.2116775512695312\n",
      "Epoch 82: loss 3.3640031814575195\n",
      "Epoch 83: loss 3.3461945056915283\n",
      "Epoch 84: loss 3.2748525142669678\n",
      "Epoch 85: loss 3.2202866077423096\n",
      "Epoch 86: loss 3.198453903198242\n",
      "Epoch 87: loss 3.195197343826294\n",
      "Epoch 88: loss 3.3735733032226562\n",
      "Epoch 89: loss 3.346543550491333\n",
      "Epoch 90: loss 3.2728216648101807\n",
      "Epoch 91: loss 3.213672637939453\n",
      "Epoch 92: loss 3.177689552307129\n",
      "Epoch 93: loss 3.1576430797576904\n",
      "Epoch 94: loss 3.328937292098999\n",
      "Epoch 95: loss 3.2884902954101562\n",
      "Epoch 96: loss 3.2208197116851807\n",
      "Epoch 97: loss 3.171374559402466\n",
      "Epoch 98: loss 3.150433301925659\n",
      "Epoch 99: loss 3.3286755084991455\n",
      "Epoch 100: loss 3.2981817722320557\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 12\n",
    "\n",
    "# Same reason as before\n",
    "if not len(X_train) % batch_size == 0:\n",
    "    raise ValueError('Select a batch size that evenly divides your data'\n",
    "                    '(or code yourself: make the final batch contain the rest'\n",
    "                    'of the elements in the set in the second loop below)')\n",
    "\n",
    "# This loop is the same as before - we just change our model.\n",
    "for epoch in range(n_epochs):\n",
    "    for batch in range(0, len(X_train), batch_size):\n",
    "        X_batch = X_train[batch:batch+batch_size]\n",
    "        y_pred = regressor_model(X_batch)\n",
    "        true_batch = y_train[batch:batch+batch_size]\n",
    "        loss = loss_fn(y_pred, true_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}: loss {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c48962d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained classifier achieved MSE 8.681280136108398 on the testing set\n"
     ]
    }
   ],
   "source": [
    "# with torch.no_grad() is technically optional here, but good practice for when using validation\n",
    "with torch.no_grad():\n",
    "    predictions = regressor_model(X_test)\n",
    "\n",
    "# This is my implementation of MSE\n",
    "errors = predictions - y_test\n",
    "mse = (errors**2).mean()\n",
    "print(f'Trained classifier achieved MSE {mse} on the testing set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98252c90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
